---
title: "Geographically Weighted Regression in R"
author: "Lex Comber and Paul Harris"
date: "15 May 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\tableofcontents

# Overview

In this session we will build on the regression work in the last session to develop a geographically weighted regression analyses. We will need to do some mapping (rather than plotting) as the analyses we will develop are spatial. This session will cover a number of areas and topics:

- Introduce Geographically Weighted Regression;
- Formally describe GWR, including the critical steps bandwidth selection and weighting;
- Apply GWR to the Liudaogou data in order to model Phosphorus;
- Map the spatially distributed coefficient estimates generated by GWR;
- Identify important further considerations when using GWR.

You will need the data and packages that you installed and used in the last R session.

# Geographically Weighted Regression: background

Geographically Weighted Regression (GWR) was first described by Brunsdon et al (1996) and then more foully covered by Fotheringham et al (2002). It has at its basis the idea that processes and relationships between variables may, in some circumstances, vary over space, and that understanding the nature of this variation can reveal important aspects of the process under consideration. 

***The*** major issue with modelling spatial data is the lack of independence of the spatial objects and the assumption of stationarity in the processes being modelled. These are well recognised in socio-economic domains, but are also relevant within bio-geographical, environmental and ecological ones and are reflected in Tobler's dictum (the First Law of Geography) that: '*everything is related to everything else, but near things are more related than distant things*' (Tobler 1970). However, despite numerous methodological advances in addressing such spatial modelling problems, whose lineage from the late 1970s can be loosely traced through spatial statistics texts from Journel and Huigbrechts (1978) to Cressie (1993), to Chil√®s and Delfiner (1999) and to Cressie and Wilke (2011), policy related research is still routinely informed by non-spatial modelling practices which are heroic at best, and ill-advised at worst. 

Focusing on regression models, the main drawback in assuming observations are independent is that any spatial dependencies turn up in the residuals, as they are not explicitly added to the model. For area-based data, ways to account for this include Besag's (1974) conditionally autoregressive model or Anselin's (1988) spatially autoregressive model. For point-based data, geostatistical regressions are commonly used, where spatial dependencies are modelled directly via the variogram with a number of recent advances (Goovaerts 2009). Spatial information can also be accounted for by expressing the regression's coefficients as functions of the spatial coordinates (Casetti 1982; Gorr and Oligschaeger 1994), permitting a model of relationship nonstationarity. GWR (Brunsdon et al. 1996) was a major advance in this area. It uses a continuous distance-decay weighting scheme to accommodate the spatial structures in the data and the resultant localised regression coefficients can be mapped. Brunsdon et al. (1998) and  Harris et al. (2015) provide some simple inferential mechanisms to determine whether the coefficients exhibit significant spatial variation. Some of the advances in the GWR method can be found in Nakaya et al. (2005), Wheeler (2007), Huang et al. (2010), Harris et al. (2011), Brunsdon et al. (2012) and Silva and Fotheringham (2015). 

In summary, a key issue in ecological modelling is that *dependence* and some form of *non-stationarity* are endemic in spatial data. When spatial non-stationarity occurs the predictor and response variables are expected to change over space and the relationship between the distribution of a certain species and predictor variables may change over space. Mechanisms to handle these characteristics of spatial data in modelling is an important challenge. This paper accommodates spatial nonstationarity using the GWR model in order to demonstrate how to address process heterogeneity in relationships. 

# GWR: formal description
In overview, GW approaches use a moving window or kernel that passes through the study area. At each location being considered, data under the window are used to make a local calculation of some kind, such as a regression. The data are weighted by their distance to the kernel centre and in this way GW approaches construct a series of models at discrete locations in the study area. 

For the geographically weighted kernels, an adaptive bi-square weighting function was applied. This generates higher weights at locations very near to the kernel centre relative to those towards the edge. For each data point ($P_j$) under the kernel with a given bandwidth, a weight $w_{i,j}$ is calculated based on its distance to the centre of the kernel ($K_{i}$) as follows:

\begin{center}
\begin{equation}
w_{i,j} = 1 - ((d_{i,j})^2 / b^2)
\end{equation}
\end{center}
where $d_{i,j}$ is the distance in metres from the centre of the kernel $K_i$ to the data point $P_j$ and $b$ is the bandwidth. 

An optimum kernel bandwidth for GWR can be found by minimising a model fit diagnostic. Options include a leave-one-out cross-validation (CV) score (Bowman 1984; Brunsdon et al. 1996). This optimises model prediction accuracy and the Akaike Information Criterion (AIC) (Akaike 1973; Fotheringham et al. 2002) optimises model parsimony by trading off prediction accuracy and complexity. 

The standard GWR model is:

\begin{center}
\begin{equation}
y_{i} = \beta_{i0} + \sum_{m}^{k=1}\beta_{ik}x_{ik} + \epsilon_i 
\end{equation}
\end{center}

where $y_i$ is the response variable at location $i$, $x_{ik}$ is the value of the $k^{th}$ predictor variable at location $i$, $m$ is the number of predictor variables, $\beta_{i0}$ is the intercept term at location $i$, $\beta_{ik}$ is the local regression coefficient for the $k^{th}$ predictor variable at location $i$ and $\epsilon_i$ is the random error at location $i$. The weighting results in data nearer to the kernel centre making a greater contribution to the estimation of local regression coefficients at each local regression calibration point $i$.

There are a number of options for weighting, as described in Gollini et al (2015). The bi-square is illustrated below:

```{r echo = F}
gw.dists.p <- function (vdist, bw, p) 
{
    cond <- which(vdist <= bw)
    wgt <- numeric(length(vdist))
    sqwgt <- 1 - vdist[cond]^p/{
        bw^p
    }
    wgt[cond] <- sqwgt^p
    if (is.matrix(vdist)) 
        wgt <- matrix(wgt, dim(vdist))
    wgt
}
d1 <- abs(rnorm(10000, 0, 750))
d1 <- sort(d1, decreasing = T)
w.d1 <- gw.dists.p(d1, 2000, 2)

plot(d1, w.d1, type = 'l', lwd = 2, 
	col = "darkred", 
	xlab = "Distance from kernel centre",
	ylab = "Weight", 
	main = "GW Bi-square Weighting \n bandwidth = 2000")
```

# GWR in R
There are a number of GWR packages for R. The best are `spgwr` and `GWmodel` and each has their own advantages:

- `GWmodel` is run by the the original GWR development team and has the widest range of GW functionality including different weighting types, distance measures, and GW operations: PCA, robust and basic GWR, ridge compensated GWR to handle local collinearity - one of the major criticisms of GWR. 
- `spgwr` has been developed by Roger Bivand and has less functionality although it runs quicker.

You should install, load and then explore both packages:

```{r eval = F}
install.packages("spgwr", dep = T)
install.packages("GWmodel", dep = T)
```

Then
```{r eval=T, echo=T, cache = T, message=F, warning=F, results="hide"}
library(GWmodel)
library(spgwr)
```


# GWR analysis 

The first thing you will need to do is to generate the `SpatialPointsDataFrame` that was called `data.sp` of the Liudaogou data in the last exercise. If you did not save this session then this can be recreated using the commands below. First download the data :

## Spatial Data
```{r eval=T, echo=T, cache = T, message=F, warning=F, results="hide"}
library(repmis)
# load the data
source_data("https://github.com/lexcomber/CAS_GW_Training/blob/master/Liudaogou.RData?raw=True")
source_data("https://github.com/lexcomber/CAS_GW_Training/blob/master/boundary.RData?raw=True")
```

Then create the spatial data. Notice that number of packages are being installed:
```{r eval=T, echo=T, cache = F, message=F, warning=F, results="hide"}
library(GISTools)
library(tidyverse)
library(plyr)
# define a projection
proj. <- CRS("+proj=tmerc +lat_0=0 +lon_0=108 +k=1 +x_0=500000 
             +y_0=0 +ellps=krass +units=m +no_defs ")
# define coordinates
coords <- data[,3:2]
# create a SpatialPointsDataFrame
data.sp <- SpatialPointsDataFrame(coords, 
    data = data.frame(data), 
    proj4string = proj.)
```

## Define the regression parameters

Now we define the regression model again based on the previous exercise, which will will then be used to help specify the GWR bandwidth.
```{r eval=T, echo=F, cache = F, message=F, warning=F, results="hide"}
terms <- names(data)[c(5,6,8:19)]
regmod <- paste(terms[2], "~")
for ( i in 2:14 ) {
	if ( i !=  2) regmod <- paste(regmod, "+", terms[i])
	if ( i == 2) regmod <- paste(regmod, terms[i])
}
regmod <- as.formula(regmod)
step.i <- stepAIC(lm(regmod, data), trace = F)
new.reg.mod <- as.formula(step.i$call)
```

The first stage in any GW analysis is to determine the size of the kernel or moving window. This is known as the kernel *bandwidth*. The bandwidth can be fixed or adaptive. A fixed bandwidth specifies a distance, and at each location for which a local analysis is being undertaken (regression in this case) data points falling within that distance are includes. An alternative is the adaptive bandwidth. Here, the nearest *n* points are included. In `spgwr` this is expressed as a proportion between [0, 1] and in `GWmodel` is expressed as an integer. The code below uses `bw.gwr` from `GWmodel` to select the bandwidth.

```{r eval=T, echo=T, cache = T, message=F, warning=F, results="hide"}
library(GWmodel)
bw <- bw.gwr(new.reg.mod, data = data.sp, 
  kernel = "bisquare", adaptive = TRUE, approach = "AIC") 
```
In this case the bandwidth is `r bw`.
```{r}
bw
```
## Run GWR 

Having determined the optimal bandwidth, this can then be used within a GWR function:
```{r eval=T, echo=T, cache = T, message=F, warning=F, results="hide"}
gwr.mod <- gwr.basic(new.reg.mod, data = data.sp, bw = bw, 
  kernel = "bisquare", adaptive = T)
```

And the outputs can be examined. Here we are interested in the coefficient estimates and how they compare to the OLS. Specifically we are interested in how the distribution of these local coefficients compare with the global ones. The `gwr` summary provides this information:
```{r, eval = F, echo = T}
summary(gwr.mod)
```
```{r, echo = F}
tmp <- (gwr.mod$SDF@data[, 1:14])
tmp2 <- t(apply(tmp, 2, function(x) fivenum(x)[2:4]))
tab <- round(cbind(gwr.mod$lm$coefficients, tmp2), 3)
colnames(tab) <- c("OLS (global)", "GWR 1stQ", "GWR median", "GWR 3rdQ")
```
```{r eval=T, fig.width=5, echo=F, message=F, warning=F}
library(knitr)
kable(tab, booktabs = T)
```

Here we can see that for some of the coefficient estimates there is considerable spatial variation. For example, consider N2P - this is the ratio of Nitrogen to Phosphorus - it varies from 0.27 to 2.40 locally , while the global value is 0.42. Other coefficient estimates do not vary so radically: for example, NO3Ngkg (Nitrate-Nitrogen) varies from 0.01 to 0.08 but proportionately this is large. So here we can see that the global model produced by the OLS regression may overestimate the association of certain covariates with the variable of interest in some places and under-estimate it in others. The next section maps these.

# Mapping the outputs

We can explore the spatial variation in the coefficient estimates. In this case the data were collected at relatively regularly spaced sample locations, making the mapping and visualizations of spatial distributions relative easy. Other options are available including mapping over regression points to generate a GWR surface. These are illustrated in the next section along with other additional GWR considerations. Here we are interested in mapping the GWR coefficient estimates. There are a number of options for doing this including 
- `choropleth` in the `GISTools` package
- the `tmap` package
- `spplot` in `sp`
You can and should explore the vignettes in R for these mapping functions but here we will stay with the `ggplot` function.

The outputs of the `gwr.basic` function are multiple but include `SDF` which is typically the same format as the input - in this case a SpatialPointsDataFrame. You will probably not want to use all of them at this stage, but instead focus on the coefficient estimates for the 14 variables used as inputs. In the code below these are written into a variable called `gwr.sp`: 
```{r}
names(gwr.mod$SDF)
class(gwr.mod$SDF)
gwr.sp <- gwr.mod$SDF[,1:14]
summary(gwr.sp)
X <- coordinates(gwr.sp)[,1]
Y <- coordinates(gwr.sp)[,2]
gwr.sp <- spCbind(gwr.sp, data.frame(X, Y))
```

We will also require the boundary data to add context to the mapping. This will need to be converted:
```{r}
boundary@data$id = rownames(boundary@data)
boundary.points = fortify(boundary, region="id")
boundary.df = join(boundary.points, boundary@data, by="id")
```

Then plotted using `ggplot`. In the below a plot has been defined that plots the `SiltPC` coefficient estimates and the variation in the degree to which changes in `SiltPC` are associated with changes  in `TPPC`:
```{r}
ggplot(boundary.df) + 
  geom_polygon(aes(x=long, y=lat), colour=NA, fill="grey") +
  coord_equal() +
  geom_point(data = gwr.sp@data, aes(x = X, y = Y, colour = SiltPC)) +
  scale_colour_gradient2()
```

**TASK:** It might be more relevant to examine the covariates that were found to be significant predictors of `TPPC`. Your task is identify these from `gwr.mod` and then to map their distributions. 

One thing you might want to to do is to consider how to map multiple outputs from `ggplot` onto a single plot window. There is a script here [http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/) that you may find useful. I modified to the code below so that it took a list of plots


```{r}
multiplot2 <- function(plot.list, file, cols=3, layout=NULL) {
  library(grid)
  # Make a list from the ... arguments and plotlist
  numPlots = length(plot.list)
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }
 if (numPlots==1) {
    print(plots[[1]])
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plot.list[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


So what you could do is to convert the ggplot routine to a function which could then be used to generate plots for all the variables.

```{r}
# plot function
my.ggplot.func <- function(i){
    p <- ggplot(boundary.df) + 
    geom_polygon(aes(x=long, y=lat), colour=NA, fill="grey") +
    coord_equal() +
    geom_point(data = gwr.sp@data, aes(x = X, y = Y, 
            colour = gwr.sp@data[i]), size = 1) +
    scale_colour_gradient2() +
    labs(subtitle = names(gwr.sp@data[i])) +
    theme(axis.title.x=element_blank(),
		        axis.text.x=element_blank(),
		        axis.ticks.x=element_blank(), 
		        axis.title.y=element_blank(),
		        axis.text.y=element_blank(),
		        axis.ticks.y=element_blank(), 
            legend.position = "none") 
  
    return(p)
}
```

These functions - `my.ggplot.func` and `multiplot2` - could be applied to all variables (but there would be 14 of them to map) or just to the significant variables. 
```{r}
p1 <- my.ggplot.func(1)
p3 <- my.ggplot.func(3)
p4 <- my.ggplot.func(4)
p5 <- my.ggplot.func(5)
p6 <- my.ggplot.func(6)
p7 <- my.ggplot.func(7)
multiplot2(list(p1,p3,p4, p5, p6, p7), cols = 3)
```

# Other Considerations
There are many many considerations that apply to regressions and to GWR - we do not have time to cover the ones we know about in a short workshop. However there are a number of things that you should be aware of that are briefly covered here 

1. It is possible to run GWR over a surface of regression points. These can be a spatial points data frame and passed to the GWR function. The resultant SDF output will over those points rather than the original spatial data. This can be good when seeking to develop a regular surface of gridded outputs.

2. GWR automatically computes Euclidean distances between input data locations. This means that toy should be aware of 2 things: If the data are in degrees then the `longlat=F` needs to be set to `TRUE` in the `gwr.basic` function so that great circle distances are used. Secondly, it is possible to pass in other distances such as network distances as well as others such as Manhattan distances. These need to be specified and passed to `dMat` parameter.

3. Collinearity amongst the predictor variables in global regressions is an issue. However, because of the way that GWR uses local subsets of the data, there is the potential for local collinearity to be present even when none is observed locally. The `GWmodel` package includes locally-compensated ridge GWR model for dealing with this: see `?gwr.lcr for a vignette on how to use it. 

# Code

All of the analyses and mappings were undertaken in R, the free open source statistical software. The RMarkdown script used to produce this manuscript, including all the code used in the analysis and to produce the mapped figures, can be found at [https://github.com/lexcomber/CAS_GW_Training](https://github.com/lexcomber/CAS_GW_Training)


# References

Akaike H (1973). Information Theory and an Extension of the Maximum Likelihood Principle. In BN Petrov, F Csaki (eds.), *2nd Symposium on Information Theory*, pp. 267‚Äì281. Akademiai Kiado, Budapest. 

Anselin L (1988). *Spatial Econometrics: Methods and Models*, Dordrecht: Kluwer Academic 

Besag, J (1974). Spatial interaction and the statistical analysis of lattice systems (with discussion). *Journal of the Royal Statistical Society B* 36: 192-225.

Brunsdon C, Charlton M, Harris P (2012). Living with Collinearity in Local Regression Models. In *Proceedings of the 10th International Symposium on Spatial Accuracy Assessment in Natural Resources and Environmental Sciences*. Brasil. 

Brunsdon C, Fotheringham AS, Charlton M (1996). Geographically Weighted Regression: A Method for Exploring Spatial Nonstationarity. *Geographical Analysis*, 28, 281‚Äì289. 

Casetti E (1982). Drift analysis of regression parameters: An application to the investigation of fertility development relations, *Modeling & Simulation* 13: 961-966.

Chiles, J.-P. and P. Delfiner (1999). *Geostatistics - modelling spatial uncertainty.*

Cressie, N (1993). *Statistics for Spatial Data*. New Jersey, John Wiley.

Cressie, N and Wilke CK (2011) *Statistics for Spatio-Temporal Data*, New Jersey, John Wiley.

Fotheringham, A. S., Brunsdon, C., & Charlton, M. (2002). Geographically weighted regression: the analysis of spatially varying relationships. John Wiley & Sons.

Gollini, I., Lu, B., Charlton, M., Brunsdon, C., & Harris, P. (2015). GWmodel: an R package for exploring spatial heterogeneity using geographically weighted models. *Journal of Statistical Software*, 63 (17), 1‚Äì50.

Goovaerts P (2009). Medical Geography: A promising field of application for geostatistics. *Mathematical Geosciences* 41: 243-264

Gorr WL, Olligschlaeger AM (1994). Weighted Spatial Adaptive Filtering: Monte Carlo Studies and Application to Illicit Drug Market Modeling. *Geographical Analysis*, 26: 67-87.

Harris P, Brunsdon C, Fotheringham AS (2011). Links, comparisons and extensions of the geographically weighted regression model when used as a spatial predictor. *Stochastic Environmental Research and Risk Assessment* 25: 123-138

Journel, A. G. and C. J. Huijbregts (1978). *Mining geostatistics*. London, Academic Press.

Nakaya, T., Fotheringham, A.S., Brunsdon, C. and Charlton, M., 2005. Geographically weighted Poisson regression for disease association mapping. *Statistics in Medicine*, 24(17), pp.2695-2717.

Tobler WR (1970). A computer movie simulating urban growth in the Detroit region. *Economic Geography*, 46: 234-240

Wheeler D (2007). Diagnostic Tools and a Remedial Method for Collinearity in Geographically Weighted Regression. *Environment and Planning A*, 39(10), 2464‚Äì2481. 


